import torch
from torch.distributions import Categorical, Normal

class DifferentiableSGD:
    """Differentiable Stochastic Gradient Descent.
    DifferentiableSGD performs the same optimization step as SGD, but instead
    of updating parameters in-place, it saves updated parameters in new
    tensors, so that the gradient of functions of new parameters can flow back
    to the pre-updated parameters.
    Args:
        module (torch.nn.module): A torch module whose parameters needs to be
            optimized.
        lr (float): Learning rate of stochastic gradient descent.
    """

    def __init__(self, module, lr=1e-3):
        self.module = module
        self.lr = lr

    def step(self):
        """Take an optimization step."""
        memo = set()

        def update(module):
            for child in module.children():
                if child not in memo:
                    memo.add(child)
                    update(child)

            params = list(module.named_parameters())
            for name, param in params:
                # Skip descendant modules' parameters.
                if '.' not in name:
                    if param.grad is None:
                        continue

                    # Original SGD uses param.grad.data
                    new_param = param.add(param.grad, alpha=-self.lr)

                    del module._parameters[name]  # pylint: disable=protected-access # noqa: E501
                    setattr(module, name, new_param)
                    module._parameters[name] = new_param  # pylint: disable=protected-access # noqa: E501

        update(self.module)

    def zero_grad(self):
        """Sets gradients of all model parameters to zero."""
        for param in self.module.parameters():
            if param.grad is not None:
                param.grad.detach_()
                param.grad.zero_()

    def set_grads_none(self):
        """Sets gradients for all model parameters to None.
        This is an alternative to `zero_grad` which sets
        gradients to zero.
        """
        for param in self.module.parameters():
            if param.grad is not None:
                param.grad = None

def update_module_params(module, new_params):  # noqa: D202
    """Load parameters to a module.
    This function acts like `torch.nn.Module._load_from_state_dict()`, but
    it replaces the tensors in module with those in new_params, while
    `_load_from_state_dict()` loads only the value. Use this function so
    that the `grad` and `grad_fn` of `new_params` can be restored
    Args:
        module (torch.nn.Module): A torch module.
        new_params (dict): A dict of torch tensor used as the new
            parameters of this module. This parameters dict should be
            generated by `torch.nn.Module.named_parameters()`
    """
    named_modules = dict(module.named_modules())

    # pylint: disable=protected-access
    def update(m, name, param):
        del m._parameters[name]  # noqa: E501
        setattr(m, name, param)
        m._parameters[name] = param  # noqa: E501

    for name, new_param in new_params.items():
        if '.' in name:
            module_name, param_name = tuple(name.rsplit('.', 1))
            if module_name in named_modules:
                update(named_modules[module_name], param_name, new_param)
        else:
            update(module, name, new_param)

def weighted_mean(tensor, dim=None, weights=None):

    if weights is None:
        out = torch.mean(tensor)
    if dim is None:
        out = torch.sum(tensor * weights)
        out.div_(torch.sum(weights))
    else:
        mean_dim = torch.sum(tensor * weights, dim=dim)
        mean_dim.div_(torch.sum(weights, dim=dim))
        out = torch.mean(mean_dim)
    return out

def weighted_normalize(tensor, dim=None, weights=None, epsilon=1e-8):
    mean = weighted_mean(tensor, dim=dim, weights=weights)
    out = tensor * (1 if weights is None else weights) - mean
    std = torch.sqrt(weighted_mean(out ** 2, dim=dim, weights=weights))
    out.div_(std + epsilon)
    return out

def detach_distribution(pi):
    if isinstance(pi, Categorical):
        distribution = Categorical(logits=pi.logits.detach())
    elif isinstance(pi, Normal):
        distribution = Normal(loc=pi.loc.detach(), scale=pi.scale.detach())
    else:
        raise NotImplementedError('Only `Categorical` and `Normal` '
                                  'policies are valid policies.')
    return distribution

def flatten_params(params):
    return torch.cat([param.view(-1) for param in params])

def assign_params_to():
    return vector_to_parameters(new_params, self.policy.parameters())

def init_gpu(use_gpu=True, gpu_id=0):
    device = None
    if torch.cuda.is_available() and use_gpu:
        device = torch.device("cuda:" + str(gpu_id))
        print("Using GPU id {}".format(gpu_id))
    else:
        device = torch.device("cpu")
        print("GPU not detected. Defaulting to CPU.")
    return device